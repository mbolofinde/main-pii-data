{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, RecognizerRegistry, EntityRecognizer, RecognizerResult\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "# Initialize Presidio Analyzer and Anonymizer engines\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# Create a custom pattern recognizer for words like \"one\", \"two\", \"three\"\n",
    "titles_recognizer = PatternRecognizer(supported_entity='NUMS', deny_list=[\"one\", \"two\", \"three\"])\n",
    "\n",
    "# Set up the registry and add predefined recognizers, then add the custom recognizer\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "registry.add_recognizer(titles_recognizer)\n",
    "analyzer = AnalyzerEngine(registry=registry)\n",
    "\n",
    "# Define the custom NumbersRecognizer class\n",
    "class NumbersRecognizer(EntityRecognizer):\n",
    "    expected_confidence_level = 0.7  # Expected confidence level for this recognizer\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyzes text to find tokens which represent numbers (either 123 or One Two Three).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # Iterate over the spaCy tokens, and call `token.like_num`\n",
    "        for token in nlp_artifacts.tokens:\n",
    "            if token.like_num:\n",
    "                result = RecognizerResult(\n",
    "                    entity_type=\"NUMBER\",\n",
    "                    start=token.idx,\n",
    "                    end=token.idx + len(token),\n",
    "                    score=self.expected_confidence_level,\n",
    "                )\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "# Initialize the custom numbers recognizer and add it to the analyzer\n",
    "new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "# Define the input and output directories\n",
    "input_directory = '/Users/mbolofinde/dev/anonprj/anonprj/output'\n",
    "output_directory = '/Users/mbolofinde/dev/anonprj/anonprj/anon_output'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Specify the PII entities you want to detect and anonymize\n",
    "entities = [\n",
    "    \"CREDIT_CARD\", \"DATE_TIME\", \"EMAIL_ADDRESS\", \"PERSON\", \"PHONE_NUMBER\",\n",
    "    \"US_BANK_NUMBER\", \"US_DRIVER_LICENSE\", \"US_SSN\", \"NUMS\", \"NUMBER\"\n",
    "]\n",
    "\n",
    "# Function to split text into smaller chunks\n",
    "def split_text(text, max_length=100000):\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# Helper function to anonymize identified PII\n",
    "def anonymize_text(text, results):\n",
    "    \"\"\"\n",
    "    Anonymize the text based on the analysis results.\n",
    "    \"\"\"\n",
    "    # Define the replacement configuration for each entity\n",
    "    operators = {\n",
    "        \"CREDIT_CARD\": OperatorConfig(\"replace\", {\"new_value\": \"<CREDIT_CARD>\"}),\n",
    "        \"DATE_TIME\": OperatorConfig(\"replace\", {\"new_value\": \"<DATE>\"}),\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"replace\", {\"new_value\": \"<EMAIL>\"}),\n",
    "        \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"<PERSON>\"}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"replace\", {\"new_value\": \"<PHONE>\"}),\n",
    "        \"US_BANK_NUMBER\": OperatorConfig(\"replace\", {\"new_value\": \"<BANK_NUMBER>\"}),\n",
    "        \"US_DRIVER_LICENSE\": OperatorConfig(\"replace\", {\"new_value\": \"<DRIVER_LICENSE>\"}),\n",
    "        \"US_SSN\": OperatorConfig(\"replace\", {\"new_value\": \"<SSN>\"}),\n",
    "        \"NUMS\": OperatorConfig(\"replace\", {\"new_value\": \"***\"}),\n",
    "        \"NUMBER\": OperatorConfig(\"replace\", {\"new_value\": \"***\"})\n",
    "    }\n",
    "\n",
    "    # Perform the anonymization using the specified operators\n",
    "    anonymized_result = anonymizer.anonymize(\n",
    "        text=text,\n",
    "        analyzer_results=results,\n",
    "        operators=operators\n",
    "    )\n",
    "    return anonymized_result.text\n",
    "\n",
    "# Process each text file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        output_file_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}_anonymized.txt\")\n",
    "        print(f'Processing file: {input_file_path}')\n",
    "        try:\n",
    "            # Read the content of the file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Split the text into smaller chunks\n",
    "            text_chunks = split_text(text)\n",
    "            anonymized_chunks = []\n",
    "\n",
    "            for chunk in text_chunks:\n",
    "                # Analyze the text chunk for PII entities\n",
    "                results = analyzer.analyze(text=chunk, entities=entities, language='en')\n",
    "                if results:\n",
    "                    print(f'Found {len(results)} PII entities in chunk. Anonymizing...')\n",
    "                    # Anonymize the detected PII entities\n",
    "                    anonymized_chunk = anonymize_text(chunk, results)\n",
    "                    anonymized_chunks.append(anonymized_chunk)\n",
    "                else:\n",
    "                    print(f'No PII entities found in chunk. Copying original content.')\n",
    "                    anonymized_chunks.append(chunk)\n",
    "\n",
    "            # Combine the anonymized chunks\n",
    "            anonymized_text = ''.join(anonymized_chunks)\n",
    "\n",
    "            # Write the anonymized text to the output file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(anonymized_text)\n",
    "            print(f'Anonymized file saved to: {output_file_path}\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {filename}: {e}\\n')\n",
    "\n",
    "print('Processing complete for all files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
